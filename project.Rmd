---
title: "What Makes A Successful Startup Company?"
author: "Lab Group - Tessa Grabowski, Himanshu Jain, Paul Noujaim, Tri Truong"
subtitle: 'Kickstarter Data Analysis'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, 
                      message = FALSE, warning = FALSE,
                      fig.height=6, fig.width = 9, fig.align = "center")
```



# Introduction

Our general research question to find out what are the most important factors
in contributing to the success of a startup project. Our dataset is comprised
of the statistics from over 300,000 Kickstarter proposals, collected directly
from the Kickstarter Platform (found on Kaggle). It includes variables that 
could be essential to determining the success of a startup such as, the amount 
of money pledged to a startup, the number of backers the project has, or the 
industry the company is in. The key variable we are looking at is the 
binary variable called "state", which shows which startups were successful 
and which were not. This will be our response variable in the analyses. We think 
this analysis of this data would be particularly useful if any member of our 
group wanted to start our own creative project by giving us an idea about which 
factors are key in indicating the future success of a startup.

Kickstarter is a global crowdfunding platform where different products can be 
listed in different categories like music, arts, technology etc. Till date, 
the company has received over $4.6 billion in funding from almost 17.2 million
backers. We believe that it would be interesting to analyze the data from this 
company to recognize the reason behind its success and how useful it might be 
for upcoming projects. Many new products are launched everyday so it would also
provide an insight to the developer as to what products would have a higher 
rate of success. 

The different variables in this dataset are- ID, name (name of the project), 
category(category of the project), main category, currency, deadline, goal 
(amount of money required), launched, pledged (amount of money the project got), 
state, backers, country, and usd pledged. We believe that some of these 
variables would be really important in providing us an insight about the 
data/company. 

# Data Analysis Plan

```{r packages}
library(tidyverse)
library(class)
library(broom)
library(usethis)
library(dplyr)
library(infer)
```
```{r reading_the_data}
kickstart <- read_csv("data/ks-projects-201612.csv")
glimpse(kickstart)
```

```{r success_rate}
kickstart%>%
  group_by(state)%>%
  filter(state=="canceled"| state=='failed'|state=='live'|state=='successful'|
           state=='suspended'|state=='undefined')%>%
  count(state)%>%
  ungroup()%>%
  mutate(rate = n/sum(n))%>%
  arrange(n)
```
From this table, we see that most of the companies have a state of either failed
or successful. Thus when we are analyzing our data, we will filter it to only 
include these companies. We are interested in what makes a project successful
versus unsuccessful, so any entry with a different state will be irrelevant to 
our analysis. We can also see that most of the companies failed so it is really
important to understand the problems in order to launch a successful campaign. 
Hence, we believe that this analysis would be helpful for future entrepreneurs.

```{r category_wise}
kickstart%>%
  group_by(category)%>%
  count()%>%
  filter(n > 100)%>%
  arrange(desc(n))
```
```{r main_categories}
kickstart%>%
  group_by(main_category)%>%
  count()%>%
  filter(n > 100)%>%
  arrange(desc(n))%>%
  ungroup()%>%
  mutate(share_of_projects = n/sum(n))
```
Looking at the numbers in the categories and main categories, we can get a
better idea at what kind of projects are more successful than others. Knowing
this will help us narrow down what the crowd is interested in investing in, and 
what fields people are more likely to succeed in if they were to create their
own project. We can see that Film and Video is the most common category and 
Product Design is the most common sub-category. We also chose to take only 
those columns that had more than 100 campaigns since the data is not clean 
and there are a lot of values that would not be allow us to present desired 
results.

The variable `pledged` tells the amount of money pledged by the crowd, or users
of the site. 

```{r pledged_needed, fig.width=9, fig.height=5}
pledged_needed <- kickstart %>%
  group_by(state) %>%
  filter(state=='failed'|state=='successful') %>%
  summarise(mean = mean(pledged), 
            min = min(pledged),
            max = max(pledged),
            goal_mean = mean(goal)) %>%
  mutate(range = max - min)

pledged_needed

kickstart_states<-kickstart%>%
  filter(state=='failed'|state=='successful')

ggplot(data = pledged_needed, mapping = aes(x = state, y = mean, 
                                            fill = state)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Successful Projects Generally Receive More Pledged Money",
       x = "State", y = "Pledged ($)", fill = "State")

ggplot(data = kickstart_states,mapping = aes(y = pledged, x = goal, 
                                           color = factor(state))) +
  geom_point(alpha = 0.3) +
  labs(color = "Goal of the Project", y = "Pledged Money") +
  theme_bw(base_size = 16)
```
This table shows that there is a large gap between the amount of money 
pledged to projects that failed as opposed to those that were successful. The
average amount of money pledged to successful projects is more than ten 
times greater than that pledged to failed projects. We can also see this in the 
bar plot above, as the bar for successful projects is significantly higher than 
the bar for failed. In addition, the range of money pledged to successful 
projects is significantly greater than the range of failed projects. We can also
see that the failed projects required much more money than the successful 
projects and even then they didn't receive much money. We believe that the 
large amount of money required makes people not pledge money for that project 
and hence, we believe that we should be having a realistic goal for the project
to be successful on kickstarter. These statistics suggest that the variable 
`pledged`has an effect on determining the state of a project.

Another variable we are interested in is `goal`. `Goal`gives the amount of money
needed by a creator to fund and finish their project. 

```{r goal_bar}
avg_goal <- kickstart %>%
  group_by(state) %>%
  filter(state=='failed'|state=='successful') %>%
  summarise(mean_goal = mean(goal))

ggplot(data = avg_goal, mapping = aes(x = state, y = mean_goal, fill = state)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Failed Projects Generally Require More Money",
       x = "State", y = "Average Goal ($)", fill = "State")
```
This visualization shows that failed projects generally were much more expensive
to create than successful projects. This is very useful in our analysis, since
it suggests that there is possibly a threshold to how much money can be required
by the creator before the project becomes unreasonable and fails. 

In our analysis it could be interesting to compare the goal of a creator to how 
much they received from pledged money as well. Looking at these two variables 
and how they interact could be very telling of the outcome of the project.

# Codebook

We can see that there are 323750 rows and 13 relevant columns in the dataset
we are using.

Variable-> Label
ID -> ID of the project that was listed
name-> Name of the Project that was listed
category->Category of the project that was listed
main_category-> Main catefgory to which the project belonged
currency-> The currency funding was requested In
deadline-> The deadline to get the required funding
goal-> The amount that was requested
launched-> The date when the funding was started
pledged-> The amount pledged by the backers
state-> The final outcome of the project
backers-> Number of people who funded the project
country-> Country where the project was launched
usd pledged-> US Dollars that the project got

Field Name->Value Label
currency 
AUD Australian Dollars
CAD Canadian Dollars
CHF Swiss Franc
DKK Danish Krone
EUR Euro
GBP Pound Sterling
MXN Mexican Peso
NOK Norwegian Krone
NZD New Zealand Dollar
SEK Swedish krona
USD US Dollar

country 
AT Austria
AU Australia
CA Canada
CH Switzerland
DE Germany
DK Denmark
ES Spain
FR France
GB United Kingdom
IE Ireland
IT Italy
MX Mexico
NL Netherlands
NO Norway
NZ New Zealand
SE Sweden
SG Singapore
US United States


# Statistical Methods 

Further, in our analysis, we will use other statistical methods to answer our 
question of what makes a project successful. We will create our own hypothetical
projects and determine statistics for the needed variables. Then we will use
the knn method to predict if this project will be successful or not based on the
data we have. 

We also will use confidence intervals for the mean amount of money required,
number of backers necessary, amount pledged, etc. that we expect a successful
company to have. 

Finally, we will use a linear regression model to figure out which variables are
the most significant in determining the eventual 'state' of the startup, 
whether they ended up being successful or unsuccessful.

Using these methods, we will hopefully be able to determine why certain projects
are successful while others are not and use this information to help us in the
future if we ever want to start our own companies.




## Clean Data 

```{r set_seed1}
set.seed(04232020)
```


```{r clean_data}
kickstart_new <- kickstart %>%
  select(-(X14:X17)) %>%
  rename(usd_pledged = 'usd pledged') %>%
  filter(state == "successful" | state == "failed", country == "US") %>%
  mutate(state = factor(state))%>%
  na.omit()

kickstart_new
```
We filtered out the data since we want to concentrate on the startups that 
either failed or were successful. There are many other states but some of them
have garbage values so we filtered the dataset. We are also looking at US 
specific startups since other countries have different startup ecosystem and 
just looking at this dataset and making conclusions for them would give us 
inaccurate results. We are basically using this to make sure that all the 
observations used have things in common.


```{r shrink_data}
kickstart_1 <- kickstart_new %>%
  group_by(state) %>%
  sample_frac(0.1) %>%
  #filter(backers <= 500) %>%
  #filter(pledged <= 25000) %>%
  ungroup()

kickstart_1
```
Since our dataset had too many observations, we decided to stick to using a 
smaller fraction of it in order to not break our program. 


```{r remove_outliers}
kickstart_out <- kickstart_1 %>%
  filter(pledged <= 11933.5) %>%
  filter(goal <= 34500) %>%
  filter(backers <= 154.4)
  
kickstart_out
```
The outliers were observed and removed to make the dataset more generic and 
to get better results. Outliers would have an effect on our results. 

```{r visualizations_minus_outliers}
ggplot(kickstart_out, mapping = aes(x = pledged)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Pledged", y = "Count")

ggplot(kickstart_out, mapping = aes(x = goal)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Goal", y = "Count")

ggplot(kickstart_out, mapping = aes(x = backers)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Backers", y = "Count")
```
The graphs clearly shows that big projects are hardly launched on kickstarter. 
It also shows that the pledged amount is not much and the number of backers 
for each project is also above 100 only for some projects. We can assume that 
smaller projects would have a higher chance of being successful on this platform
rather than the bigger ones that have a bigger goal and require more money and
backers.


```{r new_variables}
kickstart_mutated <- kickstart_1 %>%
  mutate(goal = ((goal- mean(goal)) / sd(goal)),
    pledged = ((pledged - mean(pledged)) / sd(pledged)),
    backers = ((backers - mean(backers)) / sd(backers)))
```
We standardised the variables to improve the accuracy of our KNN models. KNN 
is a distance based algorithm which is affected by the scale of the variables. 
Hence, we need to make sure that each feature has 0 mean and 1 standard 
deviation. 

## K-NN

```{r indices}
indices <- sample(nrow(kickstart_1), 2500)
```

```{r new_datasets_knn}
kickstart_train <- kickstart_mutated %>%
  slice(-indices) %>%
  select(goal, pledged,backers) 

kickstart_test <- kickstart_mutated %>%
  slice(indices) %>%
  select(goal, pledged,backers)
  
train_state <- kickstart_mutated %>%
  slice(-indices) %>%
  pull(state)

true_state <- kickstart_mutated %>%
  slice(indices) %>%
  pull(state)
```

We make the vectors required for training and testing the model. The model is 
tested on 2500 observations that we get from the dataset. 

```{r view_knn_data}
kickstart_train
kickstart_test
train_state
true_state
```


```{r knn_best}
set.seed(04272020)
i=1
k.optm=1
values <- numeric(79) 
for(i in 1:79) {
  trains_knn <- knn(kickstart_train, kickstart_test, train_state, 
                    k = (2*i)-1, prob = FALSE, use.all = FALSE)
  k.optm[i] <- 100 * sum(true_state == trains_knn)/NROW(kickstart_test)
  k=i
  cat(k,'=',k.optm[i],'')
  x <- mean(trains_knn == true_state)  
  values[i] <- x
}
tibble(values)
plot(k.optm, type="b", xlab="K- Value",ylab="Accuracy level")
which.max(values)
```
We get the highest accuracy when the value of K is 2. The accuracy is 
99% for a model like that. We can also see this from the graph. 

## Logistic Regression Model 

```{r using_variables_in_knn}
kickstart_train_log <- kickstart_mutated %>%
  slice(-indices) %>%
  select(goal, pledged, backers, state) 

m_full <- glm(state ~ goal + backers + pledged,
            data = kickstart_train_log, family = "binomial")

tidy(m_full)
```

```{r full_model}
step_full <- step(m_full, direction = "backward", trace = FALSE, 
                  results = "hide")

tidy(step_full)
```

```{r conf_int_coefficients}
#tidy(step_full, conf.int = TRUE, conf.level = 0.95)
#confint(step_full)

```
COMMENT: how do we get this function to work? Shawn said to google it but no 
luck. Anyone have any ideas?

talk about p-values 


```{r prediction_accuracy}
pred_log_odds <- augment(m_full, newdata = kickstart_test) %>% 
  mutate(prob = exp(.fitted)/ (1 + exp(.fitted))) %>%
  mutate(prob = ifelse(is.nan(prob), 1, prob)) %>%
  mutate(class = ifelse(prob >= .5, "successful", "failed")) %>%
  pull(class)

mean(true_state == pred_log_odds)
```
discuss how compares to knn test


Does category change anything?

```{r main_category_model}
m_main <- glm(state ~ goal + backers + pledged + main_category,
            data = kickstart_1, family = "binomial")

tidy(m_main)
glance(m_main)
```
Not significant so no.


## Predict Potential Project Success

```{r create_values}
mean_goal <- kickstart_1 %>%
  group_by(state) %>%
  filter(state=='successful') %>%
  summarise(mean_goal = mean(goal))

mean_pledged <- kickstart_1 %>%
  group_by(state) %>%
  filter(state=='successful') %>%
  summarise(mean_pledged = mean(pledged))

mean_backers <- kickstart_1 %>%
  group_by(state) %>%
  filter(state=='successful') %>%
  summarise(mean_backers = mean(backers))


mean_goal
mean_pledged
mean_backers
```

we need to decide specific values for our potential projects and explain 
why we chose these values

```{r predicting_project_succes}
new_project <- tibble(goal = c(1000, 9627.803, 27000),
                      backers = c(15, 253.86, 750),
                      pledged = c(925, 21480.74, 24000))

pred_project <- augment(m_full, newdata = new_project) %>% 
  pull(.fitted) 

pred_probs <- exp(pred_project) / (1 + exp(pred_project))
pred_probs

#train <- kickstart_new %>% 
  #select(goal, pledged, backers)

#success_state <- kickstart_new %>% 
  #pull(state) 

#library(class)
#mod_knn <- knn(train, new_project, success_state, k = 400, prob = F, use.all = T)
#mod_knn

```
why is it all 0?
we tried using glm model to predict since it was 0.88% more accurate,
but it doesn't do what we need it too. Values are all 0.
How do we fix this?

## ANALYSIS

-discuss outliers and need to shrink data
-discuss k-nn test and 99% accuracy result
-discuss logistic regression model result and how backers and main_category
  are not significant predictors
-compare knn and glm accuracy (99 vs 99.88)
-used glm because more accurate to predict potential project success
-explain why we chose the values of new project 
  - explain results of that and how we could use these to our benefit in the
    future 
-discuss what we could do differently
    - data set huge
    - usd_pledged
    - etc.
    
    
THINGS TO GO BACK OVER:
-organize cleaning data code 
-do we need to use outlier set for everything or can they be considered
-knitting
-VISUALIZATIONS???



