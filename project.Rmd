---
title: "What Makes A Successful Startup Company?"
author: "Lab Group - Tessa Grabowski, Himanshu Jain, Paul Noujaim, Tri Truong"
subtitle: 'Kickstarter Data Analysis'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, 
                      message = FALSE, warning = FALSE,
                      fig.height = 6, fig.width = 9, fig.align = "center")
```



# Introduction

Our general research question to find out what are the most important factors
in contributing to the success of a startup project in the United States. Our 
dataset is comprised of the statistics from over 300,000 Kickstarter proposals, 
collected directly from the Kickstarter Platform (found on Kaggle). It includes 
variables that could be essential to determining the success of a startup such 
as, the amount of money pledged to a startup, the number of backers the project
has, or the industry the company is in. The key variable we are looking at is 
the binary variable called "state", which shows which startups were successful 
and which were not. This will be our response variable in the analyses. We think 
this analysis of this data would be particularly useful if any member of our 
group wanted to start our own creative project by giving us an idea about which 
factors are key in indicating the future success of a startup.

Kickstarter is a global crowdfunding platform where different products can be 
listed in different categories like music, arts, technology etc. Till date, 
the company has received over $4.6 billion in funding from almost 17.2 million
backers. We believe that it would be interesting to analyze the data from this 
company to recognize the reason behind its success and how useful it might be 
for upcoming projects. Many new products are launched everyday so it would also
provide an insight to the developer as to what products would have a higher 
rate of success. 

The different variables in this dataset are- ID, name (name of the project), 
category(category of the project), main category, currency, deadline, goal 
(amount of money required), launched, pledged (amount of money the project got), 
state, backers, country, and usd pledged. We believe that some of these 
variables would be really important in providing us an insight about the 
data/company. 

# Data Analysis Plan

```{r packages}
library(tidyverse)
library(class)
library(broom)
library(usethis)
library(dplyr)
library(infer)
```
```{r reading_the_data}
kickstart <- read_csv("./data/kickstart_smallest.csv")
```

```{r remove_outliers}
kickstart <- kickstart %>% 
   mutate(state = as.factor(state))
kickstart
```
```{r outlier_data}
kickstart_outlier <- kickstart %>% 
  filter(pledged <= 11933.5) %>%
  filter(goal <= 34500) %>%
  filter(backers <= 154.4)
```



```{r success_rate}
kickstart %>%
  group_by(state) %>%
  count(state) %>%
  ungroup() %>%
  mutate(rate = n / sum(n)) %>%
  arrange(n)
```
From this table, we see that most of the companies have a state of either failed
or successful. Thus when we are analyzing our data, we will filter it to only 
include these companies. We are interested in what makes a project successful
versus unsuccessful, so any entry with a different state will be irrelevant to 
our analysis. We can also see that most of the companies failed so it is really
important to understand the problems in order to launch a successful campaign. 
Hence, we believe that this analysis would be helpful for future entrepreneurs.


```{r main_categories}
kickstart%>%
  group_by(main_category)%>%
  count()%>%
  filter(n > 100)%>%
  arrange(desc(n))%>%
  ungroup()%>%
  mutate(share_of_projects = n/sum(n))
```
```{r category_wise}
kickstart%>%
  group_by(category)%>%
  count()%>%
  filter(n > 100)%>%
  arrange(desc(n))
```

Looking at the numbers in the categories and main categories, we can get a
better idea at what kind of projects are more successful than others. Knowing
this will help us narrow down what the crowd is interested in investing in, and 
what fields people are more likely to succeed in if they were to create their
own project. We can see that Film and Video is the most common category and 
Product Design is the most common sub-category. We also chose to take only 
those columns that had more than 100 campaigns since the data is not clean 
and there are a lot of values that would not be allow us to present desired 
results.

The variable `pledged` tells the amount of money pledged by the crowd, or users
of the site. 

```{r pledged_needed, fig.width = 9, fig.height = 5}
pledged_needed <- kickstart %>%
  group_by(state) %>%
  summarise(mean = mean(pledged), 
            min = min(pledged),
            max = max(pledged),
            goal_mean = mean(goal)) %>%
  mutate(range = max - min)

pledged_needed


ggplot(data = pledged_needed, mapping = aes(x = state, y = mean, 
                                            fill = state)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Successful Projects Generally Receive More Pledged Money",
       x = "State", y = "Pledged ($)", fill = "State")

```
This table shows that there is a large gap between the amount of money 
pledged to projects that failed as opposed to those that were successful. The
average amount of money pledged to successful projects is more than ten 
times greater than that pledged to failed projects. We can also see this in the 
bar plot above, as the bar for successful projects is significantly higher than 
the bar for failed. In addition, the range of money pledged to successful 
projects is significantly greater than the range of failed projects. We can also
see that the failed projects required much more money than the successful 
projects and even then they didn't receive much money. We believe that the 
large amount of money required makes people not pledge money for that project 
and hence, we believe that we should be having a realistic goal for the project
to be successful on kickstarter. These statistics suggest that the variable 
`pledged`has an effect on determining the state of a project.

Another variable we are interested in is `goal`. `Goal`gives the amount of money
needed by a creator to fund and finish their project. 

```{r goal_bar}
avg_goal <- kickstart %>%
  group_by(state) %>%
  summarise(mean_goal = mean(goal))

ggplot(data = avg_goal, mapping = aes(x = state, y = mean_goal, fill = state)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Failed Projects Generally Require More Money",
       x = "State", y = "Average Goal ($)", fill = "State")
```
This visualization shows that failed projects generally were much more expensive
to create than successful projects. This is very useful in our analysis, since
it suggests that there is possibly a threshold to how much money can be required
by the creator before the project becomes unreasonable and fails. 

In our analysis it could be interesting to compare the goal of a creator to how 
much they received from pledged money as well. Looking at these two variables 
and how they interact could be very telling of the outcome of the project.

# Codebook

We can see that there are 323750 rows and 13 relevant columns in the dataset
we are using.

Variable-> Label
ID -> ID of the project that was listed
name-> Name of the Project that was listed
category->Category of the project that was listed
main_category-> Main catefgory to which the project belonged
currency-> The currency funding was requested In
deadline-> The deadline to get the required funding
goal-> The amount that was requested
launched-> The date when the funding was started
pledged-> The amount pledged by the backers
state-> The final outcome of the project
backers-> Number of people who funded the project
country-> Country where the project was launched
usd pledged-> US Dollars that the project got

Field Name->Value Label
currency 
AUD Australian Dollars
CAD Canadian Dollars
CHF Swiss Franc
DKK Danish Krone
EUR Euro
GBP Pound Sterling
MXN Mexican Peso
NOK Norwegian Krone
NZD New Zealand Dollar
SEK Swedish krona
USD US Dollar

country 
AT Austria
AU Australia
CA Canada
CH Switzerland
DE Germany
DK Denmark
ES Spain
FR France
GB United Kingdom
IE Ireland
IT Italy
MX Mexico
NL Netherlands
NO Norway
NZ New Zealand
SE Sweden
SG Singapore
US United States


# Statistical Methods 

Further, in our analysis, we will use other statistical methods to answer our 
question of what makes a project successful. We will create our own hypothetical
projects and determine statistics for the needed variables. Then we will use
the knn method to predict if this project will be successful or not based on the
data we have. 

We also will use confidence intervals for the mean amount of money required,
number of backers necessary, amount pledged, etc. that we expect a successful
company to have. 

Finally, we will use a linear regression model to figure out which variables are
the most significant in determining the eventual 'state' of the startup, 
whether they ended up being successful or unsuccessful.

Using these methods, we will hopefully be able to determine why certain projects
are successful while others are not and use this information to help us in the
future if we ever want to start our own companies.




## Clean Data 

```{r set_seed1}
set.seed(04232020)
```



We filtered out the data since we want to concentrate on the startups that 
either failed or were successful. There are many other states but some of them
have garbage values so we filtered the dataset. We are also looking at US 
specific startups since other countries have different startup ecosystem and 
just looking at this dataset and making conclusions for them would give us 
inaccurate results. We are basically using this to make sure that all the 
observations used have things in common.


Since our dataset had too many observations, we decided to stick to using a 
smaller fraction of it in order to not break our program. 


The outliers were observed and removed to make the dataset more generic and 
to get better results. Outliers would have an effect on our results. 

```{r visualizations_minus_outliers}
ggplot(kickstart_outlier, mapping = aes(x = pledged)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Pledged", y = "Count", title = "Distibution of the Amount of 
  Money Pledged to Each Project")

ggplot(kickstart_outlier, mapping = aes(x = goal)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Goal", y = "Count", title = "Distibution of the Goal Amount of 
  Money Each Project wanted to Raise")

ggplot(kickstart_outlier, mapping = aes(x = backers)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Backers", y = "Count", title = "Distibution of Backers for Each
       Project")
```
The graphs clearly shows that big projects are hardly launched on kickstarter. 
It also shows that the pledged amount is typically smaller than the goal amount
typically for larger projects. Additionally, the number of backers is only above
100 for only a small number of projects. 
```{r calc_freq_big}
kickstart %>% 
   filter(pledged >= 4000) %>% 
  count(state) %>% 
   mutate(freq = (n / sum(n)))
```

```{r calc_freq_small}
kickstart %>% 
   filter(pledged < 4000) %>% 
  count(state) %>% 
   mutate(freq = (n / sum(n)))
```
By defining a large project with one that has over $4000 pledged to it, we can
see that larger projects tend to have a much higher success rate relative to
their smaller counterparts.


```{r standardized_new_variables}
kickstart_mutated <- kickstart %>%
  mutate(
    goal = ((goal- mean(goal)) / sd(goal)),
    pledged = ((pledged - mean(pledged)) / sd(pledged)),
    backers = ((backers - mean(backers)) / sd(backers))
    )
```
We standardised the variables to improve the accuracy of our KNN models. KNN 
is a distance based algorithm which is affected by the scale of the variables. 
Hence, we need to make sure that each feature has 0 mean and 1 standard 
deviation. 

## Methods of Statistical Analysis and Results

## K-NN

```{r indices}
indices <- sample(nrow(kickstart), 880)
```

```{r new_datasets_knn}
kickstart_train <- kickstart_mutated %>%
  slice(-indices) %>%
  select(goal, pledged, backers) 

kickstart_test <- kickstart_mutated %>%
  slice(indices) %>%
  select(goal, pledged, backers)
  
train_state <- kickstart_mutated %>%
  slice(-indices) %>%
  pull(state)

true_state <- kickstart_mutated %>%
  slice(indices) %>%
  pull(state)
```

```{r knn_best}
i = 1
k.optm = 1
values <- numeric(79)
for (i in 1:79) {
  trains_knn <- knn(
    kickstart_train,
    kickstart_test,
    train_state,
    k = (2 * i) - 1,
    prob = FALSE,
    use.all = FALSE
  )
  k.optm[i] <-
    100 * sum(true_state == trains_knn) / NROW(kickstart_test)
  k = i
  cat(k, '=', k.optm[i], '')
  x <- mean(trains_knn == true_state)
  values[i] <- x
}
tibble(values) %>% 
  slice(which.max(values))
plot(k.optm,
     type = "b",
     xlab = "K- Value",
     ylab = "Accuracy level")
which.max(values) 
```

We set the indices, the number of observations to be tested on, to be 880. 

We make the vectors required for training and testing the model. We set the 
indices to be 880. The model is tested on the number of indices observations 
that we get from the dataset. 

We create two new datasets, kickstart_train and kickstart_test. kickstart_train 
contains only observations that are not in the row indices created, and 
kickstart_test contains only observations that are in the row indices created.

We create a vector of the class labels for the training dataset and call 
it train_state. We create a vector of true kickstart state in our test dataset 
and call it true_state. 

We fit the k-nearest neighbors model on our raw training dataset. We let k 
varying from 1 to 79 and calculate the value of k that results in the greatest 
prediction accuracy in our dataset along with its associated prediction 
accuracy. We then plot all the values of k and its associated prediction 
accuracy to examine the trend. 

We get the highest accuracy of 95.23% when the value of K is 1. The graph also
displays this outcome and shows that prediction accuracy tends to decrease
as k increases.


## Logistic Regression Model 

```{r logistic_model_using_same_variables_as_knn}
kickstart_train_log <- kickstart_mutated %>%
  slice(-indices) %>%
  select(goal, pledged, backers, state) 

m_full <- glm(state ~ goal + backers + pledged,
            data = kickstart_train_log, family = "binomial")

tidy(m_full)
```
$$\widehat{\mbox{state}} = (intercept) + (goal_num)~\mbox{goal} + (backers_num)~\mbox{backers} + (pledged_num)~\mbox{pledged}$$


```{r best_logistic_model}
step_full <- step(m_full, direction = "backward", trace = FALSE, 
                  results = "hide")

tidy(step_full)
```
$$\widehat{\mbox{state}} = -4121.634 -98315.777~\mbox{goal} + 
17313.216~\mbox{pledged}$$

We believe that goal and the pledged amount are the two most important variables
responsible for predicting the startups success. We started with a model that 
had backers as one of the factors but from our backward eliminiation process
we can conclude that only goal and pledged are the necessary variables.


```{r prediction_accuracy_of_log_model}
pred_log_odds <- augment(m_full, newdata = kickstart_test) %>% 
  mutate(prob = exp(.fitted)/ (1 + exp(.fitted))) %>%
  mutate(prob = ifelse(is.nan(prob), 1, prob)) %>%
  mutate(class = ifelse(prob >= .5, "successful", "failed")) %>%
  pull(class)

mean(true_state == pred_log_odds)
```

```{r regression_interaction_pledged_and_goal}
lm_interaction <- lm(pledged ~ goal, 
             data = kickstart_train_log)

lm_interaction%>%
  tidy() %>%
  select(term, estimate, p.value)
```

```{r making_a_graph, fig.width = 9, fig.height = 5}
ggplot(data = kickstart_outlier, mapping = aes(y = pledged, x = goal, 
                                           color = factor(state))) +
  geom_point(alpha = 0.3) +
  labs(title = "Gaol and Pledged Are Not Perfectly Correlated",
  color = "State of the Project", y = "Pledged Money", x = "Goal") +
  theme_bw(base_size = 16)

```



discuss how compares to knn test
We can see that the accuracy for a regression model is 99.88%, which is much 
more than the accuracy we got from our KNN model. The KNN model had the 
highest accuracy at 99%. 

## Does category change anything?

```{r main_category_model}
m_main <- glm(factor(state) ~ goal + backers + pledged + main_category,
            data = kickstart, family = "binomial")

tidy(m_main)
glance(m_main)
```
We included the categorical variable main category to check if it has any 
influence over the state of the startup. From the table above we can see that the p-values are generally
above the significance level of 0.05, so we do not consider main category as 
a major predictor of success.

## Predict Potential Project Success

```{r create_values}
median_goal <- kickstart_mutated %>%
  group_by(state) %>%
  filter(state =='successful') %>%
  summarise(med_goal = median(goal))

median_pledged <- kickstart_mutated %>%
  group_by(state) %>%
  filter(state =='successful') %>%
  summarise(med_pledged = median(pledged))

median_backers <- kickstart_mutated %>%
  group_by(state) %>%
  filter(state =='successful') %>%
  summarise(med_backers = median(backers))


median_goal
median_pledged
median_backers
```


```{r predicting_project_succes}
new_project <- tibble(#goal = c(1000, 1000, 9709.67, 27000, 27000),
                      #backers = c(5, 70, 240.6017, 7500, 7500),
                      #pledged = c(30, 30, 19071.34, 45000, 400))
                      goal = c(-0.05, -0.05, -0.0308734, 2, 2),
                      backers = c(-0.1, 4, -0.06979549, 0.8, 0.8),
                      pledged = c(-0.5, -0.5, -0.07385757, 4, 3))

#pred_project <- augment(m_full, newdata = new_project) %>% 
  #pull(.fitted) 

#pred_probs <- exp(pred_project) / (1 + exp(pred_project))
#pred_probs

train <- kickstart_mutated %>% 
  select(goal, pledged, backers)

success_state <- kickstart_mutated %>% 
  pull(state) 

mod_knn <- knn(train, new_project, success_state, k = 1, prob = F, use.all = T)
mod_knn

```


## ANALYSIS




We use this prediction accuracy to predict the project's success. Given a new
start-up project, we plot that project acordingly to its attributes, check 
for the nearest neightbor of the project, and assign that project to the 
according state. Using this method, the prediction accuracy is 95.23%, which is 
the highest prediction accuracy we can get.



-discuss outliers and need to shrink data
-discuss k-nn test and 99% accuracy result
-discuss logistic regression model result and how backers and main_category
  are not significant predictors
-compare knn and glm accuracy (99 vs 99.88)
-used glm because more accurate to predict potential project success
-explain why we chose the values of new project 
  - explain results of that and how we could use these to our benefit in the
    future 
-discuss what we could do differently
    - data set huge
    - had to make changes, cannot predict for bigger values after we removed
    outliers
    - usd_pledged
    - used knn instead 
    - etc.
    
  - success rates of big projects vs. smaller ones
  - remove outliers for visualization purposes
  
  further analysis:
  - use interaction between pledged and goal
  
THINGS TO GO BACK OVER:
-organize cleaning data code 
-do we need to use outlier set for everything or can they be considered
-knitting
-VISUALIZATIONS???





