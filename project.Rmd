---
title: "What Makes A Successful Project Idea in the USA?"
author: "Lab Group - Tessa Grabowski, Himanshu Jain, Paul Noujaim, Tri Truong"
subtitle: 'Kickstarter Data Analysis'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, 
                      message = FALSE, warning = FALSE,
                      fig.height = 6, fig.width = 9, fig.align = "center")
```



# Introduction

Our general research question to find out what are the most important factors
in contributing to the success of a startup project in the United States. Our 
dataset is comprised of the statistics from over 300,000 Kickstarter proposals, 
collected directly from the Kickstarter Platform (found on Kaggle). It includes 
variables that could be essential to determining the success of a startup such 
as, the amount of money pledged to a startup, the number of backers the project
has, or the industry the company is in. The key variable we are looking at is 
the binary variable called "state", which shows which startups were successful 
and which were not. This will be our response variable in the analyses. We think 
this analysis of this data would be particularly useful if any member of our 
group wanted to start our own creative project by giving us an idea about which 
factors are key in indicating the future success of a startup.

Kickstarter is a global crowdfunding platform where different products can be 
listed in different categories like music, arts, technology etc. Till date, 
the company has received over $4.6 billion in funding from almost 17.2 million
backers. We believe that it would be interesting to analyze the data from this 
company to recognize the reason behind its success and how useful it might be 
for upcoming projects. Many new products are launched everyday so it would also
provide an insight to the developer as to what products would have a higher 
rate of success. 

The different variables in this dataset are- ID, name (name of the project), 
category(category of the project), main category, currency, deadline, goal 
(amount of money required), launched, pledged (amount of money the project got), 
state, backers, country, and usd pledged. We believe that some of these 
variables would be really important in providing us an insight about the 
data/company. 

# Data Analysis Plan

```{r packages}
library(tidyverse)
library(class)
library(broom)
library(usethis)
library(dplyr)
library(infer)
```
```{r reading_the_data}
kickstart <- read_csv("./data/kickstart_smallest.csv")
```

```{r remove_outliers}
kickstart <- kickstart %>% 
   mutate(state = as.factor(state))
kickstart
```

```{r outlier_data}
kickstart_outlier <- kickstart %>% 
  filter(pledged <= 11933.5) %>%
  filter(goal <= 34500) %>%
  filter(backers <= 154.4)
```


```{r main_categories}
kickstart%>%
  group_by(main_category)%>%
  count()%>%
  filter(n > 100)%>%
  arrange(desc(n))%>%
  ungroup()%>%
  mutate(share_of_projects = n/sum(n))
```

```{r category_wise}
kickstart%>%
  group_by(category)%>%
  count()%>%
  filter(n > 100)%>%
  arrange(desc(n))
```

Looking at the numbers in the categories and main categories, we can get a
better idea at what kind of projects are more successful than others. Knowing
this will help us narrow down what the crowd is interested in investing in, and 
what fields people are more likely to succeed in if they were to create their
own project. We can see that Film and Video is the most common category and 
Product Design is the most common sub-category. We also chose to take only 
those columns that had more than 100 campaigns since the data is not clean 
and there are a lot of values that would not be allow us to present desired 
results.

The variable `pledged` tells the amount of money pledged by the crowd, or users
of the site. 

```{r pledged_needed, fig.width = 9, fig.height = 5}
pledged_needed <- kickstart %>%
  group_by(state) %>%
  summarise(mean = mean(pledged), 
            min = min(pledged),
            max = max(pledged),
            goal_mean = mean(goal)) %>%
  mutate(range = max - min)

pledged_needed


ggplot(data = pledged_needed, mapping = aes(x = state, y = mean, 
                                            fill = state)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Successful Projects Generally Receive More Pledged Money",
       x = "State", y = "Pledged ($)", fill = "State")

```
This table shows that there is a large gap between the amount of money 
pledged to projects that failed as opposed to those that were successful. The
average amount of money pledged to successful projects is more than ten 
times greater than that pledged to failed projects. We can also see this in the 
bar plot above, as the bar for successful projects is significantly higher than 
the bar for failed. In addition, the range of money pledged to successful 
projects is significantly greater than the range of failed projects. We can also
see that the failed projects required much more money than the successful 
projects and even then they didn't receive much money. We believe that the 
large amount of money required makes people not pledge money for that project 
and hence, we believe that we should be having a realistic goal for the project
to be successful on kickstarter. These statistics suggest that the variable 
`pledged`has an effect on determining the state of a project.

Another variable we are interested in is `goal`. `Goal`gives the amount of money
needed by a creator to fund and finish their project. 

```{r goal_bar}
avg_goal <- kickstart %>%
  group_by(state) %>%
  summarise(mean_goal = mean(goal))

ggplot(data = avg_goal, mapping = aes(x = state, y = mean_goal, fill = state)) +
  geom_bar(stat = "identity") +
  theme_bw() +
  labs(title = "Failed Projects Generally Require More Money",
       x = "State", y = "Average Goal ($)", fill = "State")
```
This visualization shows that failed projects generally were much more expensive
to create than successful projects. This is very useful in our analysis, since
it suggests that there is possibly a threshold to how much money can be required
by the creator before the project becomes unreasonable and fails. 

In our analysis it could be interesting to compare the goal of a creator to how 
much they received from pledged money as well. Looking at these two variables 
and how they interact could be very telling of the outcome of the project.

```{r set_seed1}
set.seed(04232020)
```


```{r visualizations_minus_outliers}
ggplot(kickstart_outlier, mapping = aes(x = pledged)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Pledged", y = "Count", title = "Distibution of the Amount of 
  Money Pledged to Each Project")

ggplot(kickstart_outlier, mapping = aes(x = goal)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Goal", y = "Count", title = "Distibution of the Goal Amount of 
  Money Each Project wanted to Raise")

ggplot(kickstart_outlier, mapping = aes(x = backers)) +
  geom_histogram(alpha = .5, color = "darkgreen") +
  theme_bw() +
  labs(x = "Backers", y = "Count", title = "Distibution of Backers for Each
       Project")
```

By defining a large project with one that has over $4000 pledged to it, 
the graphs show that big projects are launched less often than 
smaller projects on kickstarter. 
It also shows that the pledged amount is typically smaller than the goal amount
for larger projects. 
Additionally, the number of backers is only above 100 for only a small 
number of projects. 

```{r calc_freq_big}
kickstart %>% 
   filter(pledged >= 4000) %>% 
  count(state) %>% 
   mutate(freq = (n / sum(n)))
```

```{r calc_freq_small}
kickstart %>% 
   filter(pledged < 4000) %>% 
  count(state) %>% 
   mutate(freq = (n / sum(n)))
```

We can see that larger projects tend to have a much higher success rate (85.4%)
relative to their smaller counterparts (24.8%).


# Codebook

We can see that there are 11468 rows and 13 relevant columns in the dataset
we are using.

Variable-> Label
`ID` -> ID of the project that was listed
`name`-> Name of the Project that was listed
`category`->Category of the project that was listed
`main_category`-> Main catefgory to which the project belonged
`currency`-> The currency funding was requested In
`deadline`-> The deadline to get the required funding
`goal`-> The amount that was requested
`launched`-> The date when the funding was started
`pledged`-> The amount pledged by the backers
`state`-> The final outcome of the project
`backers`-> Number of people who funded the project
`country`-> Country where the project was launched
`usd pledged`-> US Dollars that the project got


# Statistical Methods 

In considering the state of a project, we hypothesize that goal, pledged, 
backers, and main category will be the most significant predictors of whether
or not a project succeeds.

Further, in our analysis, we will use other statistical methods to answer our 
question of what makes a project successful. We will create our own hypothetical
projects and determine statistics for the needed variables. Then we will use
the knn method to predict if this project will be successful or not based on the
data we have. 

Finally, we will use a logistic regression model to figure out which variables
are the most significant in determining the eventual 'state' of the startup, 
whether they ended up being successful or unsuccessful.

Using these methods, we will hopefully be able to determine why certain projects
are successful while others are not and use this information to help us in the
future if we ever want to start our own projects.


## Standardize Data 

```{r standardized_new_variables}
kickstart_mutated <- kickstart %>%
  mutate(
    goal = ((goal- mean(goal)) / sd(goal)),
    pledged = ((pledged - mean(pledged)) / sd(pledged)),
    backers = ((backers - mean(backers)) / sd(backers))
    )
```

We standardized the variables to improve the accuracy of our K-NN models. K-NN 
is a distance based algorithm which is affected by the scale of the variables. 
Hence, we need to make sure that each feature has 0 mean and 1 standard 
deviation. 

## METHODS AND RESULTS

We filtered out the data since we want to concentrate on the startups that 
either failed or were successful. There are many other states but some of them
have insignificant values so we filtered the dataset. We are also looking at the 
US specific startups since other countries have different startup ecosystem and 
just looking at this dataset and making conclusions for them would give us 
inaccurate results. We are basically using this to make sure that all the 
observations used orriginate from the same country.

Since our dataset had too many observations, we decided to stick to using a 
smaller fraction of it in order to not break our program. 

The outliers were observed and removed to make the visualizations more generic 
and to get better results. Outliers would have an effect on how the data is
displayed. 

## Main Category
For all the linear models we make, we assume that all observations are 
independent on one another. 

```{r main_category_model}
m_main <- glm(factor(state) ~ goal + backers + pledged + main_category,
            data = kickstart, family = "binomial")

tidy(m_main) %>%
  select(term, estimate, p.value)
```

In order to ensure that, we have the best fit model, we tested to 
see if it had any effect on the outcome of state. After fitting a linear 
regression model, it was observed that main category did not have a 
statistically significant effect, since each category’s p-value was greater 
than our alpha value of 0.05. Therfore, we do not consider the main category
variable in further models.

## K-NN

```{r indices}
indices <- sample(nrow(kickstart), 880)
```

```{r new_datasets_knn}
kickstart_train <- kickstart_mutated %>%
  slice(-indices) %>%
  select(goal, pledged, backers) 

kickstart_test <- kickstart_mutated %>%
  slice(indices) %>%
  select(goal, pledged, backers)
  
train_state <- kickstart_mutated %>%
  slice(-indices) %>%
  pull(state)

true_state <- kickstart_mutated %>%
  slice(indices) %>%
  pull(state)
```

```{r knn_best}
i = 1
k.optm = 1
values <- numeric(79)
for (i in 1:79) {
  trains_knn <- knn(
    kickstart_train,
    kickstart_test,
    train_state,
    k = (2 * i) - 1,
    prob = FALSE,
    use.all = FALSE
  )
  k.optm[i] <-
    100 * sum(true_state == trains_knn) / NROW(kickstart_test)
  k = i
  x <- mean(trains_knn == true_state)
  values[i] <- x
}
tibble(values) %>% 
  slice(which.max(values))
plot(k.optm,
     type = "b",
     xlab = "K- Value",
     ylab = "Accuracy level")
which.max(values) 
```

We set the indices, the number of observations to be tested on, to be 880. 

We made the vectors required for training and testing the model. The model is 
tested on the number of observations that we get from the dataset. 

We created two new datasets, kickstart_train and kickstart_test using the 
three predicting numeric variables (goal, pledged, backers) for the categorical 
variable state. 
kickstart_train contains only observations that are not in the row indices 
created, and kickstart_test contains only observations that are in the row 
indices created.

We created a vector of the class labels for the training dataset and called 
it train_state. We created a vector of true kickstart state in our test dataset 
and called it true_state. 

We fit the k-nearest neighbors model on our raw training dataset. We let k 
vary from 1 to 79 and calculated the value of k that results in the greatest 
prediction accuracy in our dataset along with its associated prediction 
accuracy. We then plotted all the values of k and its associated prediction 
accuracy to examine the trend. 

We get the highest accuracy of 95.23% when the value of K is 1. The graph also
displays this outcome and shows that prediction accuracy tends to decrease
as k increases, suggesting that backers, pledged, and goal are effective 
predictors of state according to the k-NN model.

We use this prediction accuracy to predict the project's success. Given a new
start-up project, we plot that project acordingly to its attributes, check 
for the nearest neightbor of the project, and assign that project to the 
according state. 


## Logistic Regression Model 

Let $\alpha = 0.05:$

```{r logistic_model_using_same_variables_as_knn}
kickstart_train_log <- kickstart_mutated %>%
  slice(-indices) %>%
  select(goal, pledged, backers, state) 

m_full <- glm(state ~ goal + backers + pledged,
            data = kickstart_train_log, family = "binomial")

tidy(m_full) %>%
  select(term, estimate, p.value)
```

The coefficient for pledged means that, holding all other variables constant, 
for each dollar increase in money pledged, we would expect the log-odds of a 
start-up being successful to increase on average by approximately 5.32*10^4.

The coefficient for goal means that, holding all other variables constant, 
for each dollar increase in the goal amount of money a company would like to 
receive, we would expect the log-odds of a start-up being successful to 
decrease on average by approximately 1.45*10^6.

Since, the p-values for money pledged and goal are 1.56*10^-6 and 1.55*10^-6 
respectively, that means both variables are significant in predicting the state
of a start-up at the alpha = .05 level. 

However, since the p-value for backers is 7.97*10^-1, the variable is not a 
significant predictor of the state of a start-up at the alpha = .05 level. 

It is important to note that these two variables share an 
opposite relationship with the state variable. Pledged has a positive 
relationship with state, meaning that with each increase in the amount of
dollars pledged to a start-up will increase the probability that it will become 
successful. Whereas, goal shares a negative relationship with state, meaning 
that with each increase in the amount of dollars a start-up requires will 
decrease the probability that it will become successful.


```{r best_logistic_model}
step_full <- step(m_full, direction = "backward", trace = FALSE, 
                  results = "hide")

tidy(step_full) %>%
  select(term, estimate, p.value)
```

We believe that goal and the pledged amount are the two most important variables
responsible for predicting the startups success. We started with a model that 
had backers as one of the factors but from our backward eliminiation process
we can conclude that only goal and pledged are the necessary variables.


```{r prediction_accuracy_of_log_model}
pred_log_odds <- augment(m_full, newdata = kickstart_test) %>% 
  mutate(prob = exp(.fitted)/ (1 + exp(.fitted))) %>%
  mutate(prob = ifelse(is.nan(prob), 1, prob)) %>%
  mutate(class = ifelse(prob >= .5, "successful", "failed")) %>%
  pull(class)

result_pred_log_odds <- mean(true_state == pred_log_odds)
result_pred_log_odds
```

Using the same variables as our k-NN model, we can see that the accuracy for the 
regression model predicting state is 100%, which is slightly higher than the 
accuracy we got from our K-NN model, which had the highest accuracy at 95.23%.
This suggests that the regression model is better at predicting state given 
the same variables.

```{r regression_interaction_pledged_and_goal}
lm_interaction <- lm(pledged ~ goal, 
             data = kickstart_train_log)

tidy(lm_interaction) %>%
  select(term, estimate, p.value)
```
```{r conditions_check_independence}
full_model <- augment(lm_interaction)
ggplot(data = lm_interaction, aes(x = 1:nrow(full_model), y = .resid)) +
geom_point() +
labs(x = "Index", y = "Residual", title="Residuals vs Index",
 subtitle = "Both the variables don't seem independent")+
  theme_minimal(base_size = 16)
```

```{r residual_variance}
ggplot(data = full_model, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, color = "gray") +
  labs(title="Residuals vs the fitted value", y = "Residuals", 
  x = expression(hat(y)), 
  subtitle="We don't see constant variance and distribution to be around 0") +
  theme_minimal(base_size = 16)
```

```{r histogram}
ggplot(data = full_model, aes(x = .resid)) +
  geom_histogram() +
  labs(title="Count vs Residuals", 
  subtitle="The histogram is not centered around 0",
  x = "Residuals", y = "Count") +
  theme_minimal(base_size = 16)
```
We can see that none of the linear model assumptions are being satisfied 
by this model but we assumed them to get predictions for our dataset. 

```{r making_a_graph, fig.width = 9, fig.height = 5}
ggplot(data = kickstart_outlier, mapping = aes(y = pledged, x = goal, 
                                           color = factor(state))) +
  geom_point(alpha = 0.3) +
  labs(title = "Goal and Pledged Are Not Perfectly Correlated",
  color = "State of the Project", y = "Pledged Money", x = "Goal") +
  theme_bw(base_size = 16)
```

A linear model predicting the amount of money pledged from the goal, shows
that goal is not a strong predictor of pledged. Just because a project demands 
a certain sum of money does not imply that they will receive it, as seen in 
the visualization above in most failed projects. We can also see that the slope
of successful campaigns is either 1 or greater than that. 

## Predict Potential Project Success

```{r create_values}
median_goal <- kickstart_mutated %>%
  group_by(state) %>%
  filter(state =='successful') %>%
  summarise(med_goal = median(goal))

median_pledged <- kickstart_mutated %>%
  group_by(state) %>%
  filter(state =='successful') %>%
  summarise(med_pledged = median(pledged))

median_backers <- kickstart_mutated %>%
  group_by(state) %>%
  filter(state =='successful') %>%
  summarise(med_backers = median(backers))

median_goal
median_pledged
median_backers
```

We will use these values as a benchmark when testing potential hypothetical
projects.

```{r predicting_project_success_with_standardized_values}
new_project <- tibble(goal = c(-0.05, -0.05, -0.0308734, 2, 2),
                      backers = c(-0.1, 4, -0.06979549, 0.8, 0.8),
                      pledged = c(-0.5, -0.5, -0.07385757, 4, 3))

train <- kickstart_mutated %>% 
  select(goal, pledged, backers)

success_state <- kickstart_mutated %>% 
  pull(state) 

mod_knn <- knn(train, new_project, success_state, k = 1, prob = F, use.all = T)
mod_knn
```

We wanted to predict a potential project’s success, simulating what it would be 
like if we were to create our own individual projects. 

We proposed five possible projects with different values for goal, pledged, 
and backers. Our third project used the median values for each of the three 
variables to see how the average project proposed on kickstarter would play out. 
We used the median value instead of the mean since our data had outliers 
present, which would skew the mean value. All the other proposed projects had 
considerably random values chosen. 

We used the k-NN test to predict the state of these new projects, and the 
median project resulted in success. This means that the average project 
compared to the 1 nearest neighbor is 95.23 % likely to be successful.

The first two projects had the same random values for goal and pledged, both 
being lower than the respective median value. However, the backers for both was
varied to see if the number of backers would change the outcome with all else 
constant. The second project had a much larger value for backers, but it still 
resulted in failure like the first. This is consistent with what we observed in
our logistic regression model, which showed that backers does not have a 
significant effect in predicting the outcome of a project.

Our last two projects had random values for goal and backers, both being 
greater than the respective median value. However, the pledged for both was 
different to see if the amount pledged would change the outcome with all else 
constant. The last project had a lower value for amount pledged and resulted in
a failure. The fourth project resulted in success though, which implies that 
the amount pledged is important to determining the state of the project.


## DISCUSSION

Through our analysis, we were able to determine that the variables `goal` and 
`pledged` are the most essential factors in deciding the outcome of a project,
either successful or failed. This is different than what our hypothesis stated,
which predicted that `goal`, `pledged`, `backers`, and `main_category` would 
all be predictors of the outcome of state.

First, we eliminated `main_category` as a predictor by fitting a linear 
regression model and observing the p-values of each main category to be 
insignificant at our alpha level of 0.05.

Next, using our k-NN model, we determined that we could predict with a 95.23 % 
accuracy the state of a project given only the variables `goal`, `pledged`, 
and `backers` when the number of nearest neighbors is equal to 1. While 1 seems
like a very small number of observations to be compared to, this can be
explained due to our high number of observations in general and the trends 
detected. Given the visualization that plots the relationship between `goal` 
and `pledged`, we see that there is a very sharp distinction between the 
projects that have succeeded or failed based on their pledged and goal values. 
Therefore, a project's nearest neighbor most likely has a high chance of 
having the same state if pledged and goal values are very similar. We found
out through our logistic model next that `backers` is less essential to
predicting state.

The logistic model further reinforced the value of the variables `pledged` and 
`goal` at predicting the state of a project. It revealed the variables
pledged and goal to be extremely significant in predicting the state of a given
project at the alpha = .05 level. Their p-values being 
1.56*10^-6 and 1.55*10^-6 respectively, meaning that these two variables are 
essential in determining if your project will be a success or a failure, while 
backers is not. Backers was shown to be an 
insignificant predictor of state with a p-value of only 7.97 *10^-1 meaning 
the variable has limited capacity to predict the state of a company. 
In essence, when beginning a start-up the amount of money pledged to one’s 
startup in relation to its goal amount will be the ultimate determinate of 
success. 

Despite the ineffectiveness of backers as a predictor of state, when using 
the logistic model to predict the success of a project, we had to consider 
`backers` still in order to provide an equal comparison between the k-NN model 
and the logistic model. According to these three variables, the logistic 
regression model predicts the state of a project with 100 % accuracy,
further proving the importance of `goal` and `pledged` as predictors, while 
also suggesting that `backers` may have some effect even if it may be small.

Finally, we created a model using the k-NN test to determine the success of 
hypothetical projects. In doing so, we further disproved the value of 
`backers` as significant in predicting state, since when both pledged and goal
were held constant in two different potential projects, a large increase 
in backers did not cause the state to change to be successful. However, when
goal and backers were held constant and even a relatively small increase in 
pledged occurred between two projects, the state changed from failed to 
successful. 

In retrospect, there are a few things we could have done differently to improve
the results of our analysis. First, our original data set included 280,000 plus
observations, which caused us problems in trying to perform tests. Although we
took a smaller sample of this data, it might have been better to have chosen
a dataset that was smaller from the start that would be easier to use in
computation. Additionally, the data spans over the years of 2010 to 2016,
which may be slightly outdated for predicting a project that would
be successful in the present, since it is possible that trends have changed
since then. Finally, in predicting the outcome of our hypothetical projects,
we decided to use the k-NN model over the logistic regression model even
though the logistic model had a higher prediction accuracy. This was due 
to coding errors in R. When the model tried to predict the outcome of the
inputted values, we continuously got values of zero percent success rate even 
when we knew this was not the case. Thus, using the logistic regression model 
would have in theory been more accurate, but we were unable to do so. The k-NN
model, however, still had a high prediction accuracy, so our predicted project
outcomes are still likely valid. As we saw above, the linear model is not 
satisfying the conditions required so we believe making a better model is 
using that as a predictor would be something we would like to do next time.

If we wanted to further explore this analysis, we could take into account 
projects proposed outside of the US and see if our prediction variables still
hold the same value in determining state. In this situation, we would need
to use the usd_pledged variable instead of the pledged variable in order 
standardize the currency in US dollars. Additionally, we could make use of the
two variables deadline and launched to determine whether or not time elapsed 
from start to deadline is a determining factor of success. For example, if
projects with a short time between launch and deadline tend to have a 
higher chance of success compared to those with longer elapsed time. 


 
  
  





